{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate \n",
    "#bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install C:/triton-2.1.0-cp310-cp310-win_amd64.whl\n",
    "!pip install C:/bitsandbytes-0.43.0.dev0-cp310-cp310-win_amd64.whl\n",
    "!pip install C:/deepspeed-0.13.1+unknown-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerate==0.30.1\n",
      "aiohttp==3.9.5\n",
      "aiosignal==1.3.1\n",
      "asttokens @ file:///home/conda/feedstock_root/build_artifacts/asttokens_1698341106958/work\n",
      "async-timeout==4.0.3\n",
      "attrs==23.2.0\n",
      "backcall @ file:///home/conda/feedstock_root/build_artifacts/backcall_1592338393461/work\n",
      "bitsandbytes==0.43.1\n",
      "certifi==2024.2.2\n",
      "charset-normalizer==3.3.2\n",
      "colorama @ file:///home/conda/feedstock_root/build_artifacts/colorama_1666700638685/work\n",
      "comm @ file:///home/conda/feedstock_root/build_artifacts/comm_1710320294760/work\n",
      "datasets==2.19.1\n",
      "debugpy @ file:///D:/bld/debugpy_1707444604970/work\n",
      "decorator @ file:///home/conda/feedstock_root/build_artifacts/decorator_1641555617451/work\n",
      "dill==0.3.8\n",
      "einops==0.8.0\n",
      "executing @ file:///home/conda/feedstock_root/build_artifacts/executing_1698579936712/work\n",
      "faiss==1.7.4\n",
      "filelock==3.14.0\n",
      "frozenlist==1.4.1\n",
      "fsspec==2024.3.1\n",
      "huggingface-hub==0.23.1\n",
      "idna==3.7\n",
      "ijson==3.2.3\n",
      "importlib_metadata @ file:///home/conda/feedstock_root/build_artifacts/importlib-metadata_1710971335535/work\n",
      "intel-openmp==2021.4.0\n",
      "ipykernel @ file:///D:/bld/ipykernel_1708996677248/work\n",
      "ipython @ file:///D:/bld/ipython_1680185618122/work\n",
      "jedi @ file:///home/conda/feedstock_root/build_artifacts/jedi_1696326070614/work\n",
      "Jinja2==3.1.4\n",
      "joblib==1.4.2\n",
      "jupyter_client @ file:///home/conda/feedstock_root/build_artifacts/jupyter_client_1716472197302/work\n",
      "jupyter_core @ file:///D:/bld/jupyter_core_1710257377578/work\n",
      "MarkupSafe==2.1.5\n",
      "matplotlib-inline @ file:///home/conda/feedstock_root/build_artifacts/matplotlib-inline_1713250518406/work\n",
      "mkl==2021.4.0\n",
      "mpmath==1.3.0\n",
      "multidict==6.0.5\n",
      "multiprocess==0.70.16\n",
      "nest_asyncio @ file:///home/conda/feedstock_root/build_artifacts/nest-asyncio_1705850609492/work\n",
      "networkx==3.2.1\n",
      "numpy @ file:///D:/bld/numpy_1707225561314/work/dist/numpy-1.26.4-cp39-cp39-win_amd64.whl#sha256=af5f40857bb7ceb2d3be562fa763863d2f954c104873e89b8ee370f75e047ecf\n",
      "packaging @ file:///home/conda/feedstock_root/build_artifacts/packaging_1710075952259/work\n",
      "pandas==2.2.2\n",
      "parso @ file:///home/conda/feedstock_root/build_artifacts/parso_1712320355065/work\n",
      "pickleshare @ file:///home/conda/feedstock_root/build_artifacts/pickleshare_1602536217715/work\n",
      "pillow==10.3.0\n",
      "platformdirs @ file:///home/conda/feedstock_root/build_artifacts/platformdirs_1715777629804/work\n",
      "polars==0.20.29\n",
      "prompt-toolkit @ file:///home/conda/feedstock_root/build_artifacts/prompt-toolkit_1702399386289/work\n",
      "psutil @ file:///D:/bld/psutil_1705722541723/work\n",
      "pure-eval @ file:///home/conda/feedstock_root/build_artifacts/pure_eval_1642875951954/work\n",
      "pyarrow==16.1.0\n",
      "pyarrow-hotfix==0.6\n",
      "Pygments @ file:///home/conda/feedstock_root/build_artifacts/pygments_1714846767233/work\n",
      "python-dateutil @ file:///home/conda/feedstock_root/build_artifacts/python-dateutil_1709299778482/work\n",
      "pytz==2024.1\n",
      "pywin32==306\n",
      "PyYAML==6.0.1\n",
      "pyzmq @ file:///D:/bld/pyzmq_1715024536945/work\n",
      "regex==2024.5.15\n",
      "requests==2.32.2\n",
      "safetensors==0.4.3\n",
      "scikit-learn==1.5.0\n",
      "scipy==1.13.1\n",
      "sentence-transformers==2.7.0\n",
      "six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work\n",
      "stack-data @ file:///home/conda/feedstock_root/build_artifacts/stack_data_1669632077133/work\n",
      "sympy==1.12\n",
      "tbb==2021.12.0\n",
      "threadpoolctl==3.5.0\n",
      "tokenizers==0.19.1\n",
      "torch==2.3.0\n",
      "tornado @ file:///D:/bld/tornado_1708363257374/work\n",
      "tqdm==4.66.4\n",
      "traitlets @ file:///home/conda/feedstock_root/build_artifacts/traitlets_1713535121073/work\n",
      "transformers==4.41.1\n",
      "typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1712329955671/work\n",
      "tzdata==2024.1\n",
      "urllib3==2.2.1\n",
      "wcwidth @ file:///home/conda/feedstock_root/build_artifacts/wcwidth_1704731205417/work\n",
      "xxhash==3.4.1\n",
      "yarl==1.9.4\n",
      "zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1695255097490/work\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\unsloth_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.43.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3060 Ti. Max memory: 7.999 GB. Platform = Windows.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: unsloth/tinyllama-bnb-4bit can only handle sequence lengths of at most 2048.\n",
      "But with kaiokendev's RoPE scaling of 2.0, it can be magically be extended to 4096!\n",
      "d:\\anaconda3\\envs\\unsloth_env\\lib\\site-packages\\unsloth\\models\\llama.py:1022: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  self.register_buffer(\"cos_cached\", emb.cos().to(dtype=dtype, device=device, non_blocking=True), persistent=False)\n",
      "Unsloth 2024.8 patched 22 layers with 22 QKV layers, 22 O layers and 22 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    # model_name = checkpoint_dir + \"/litm_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = 4096,\n",
    "    dtype = None,\n",
    "    model_name = \"Paoloc99/litm_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "eval_dataset = load_dataset(\"Paoloc99/dataset\", split=\"test[:100]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]d:\\anaconda3\\envs\\unsloth_env\\lib\\site-packages\\unsloth\\models\\llama.py:407: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  A = scaled_dot_product_attention(Q, K, V, attn_mask = attention_mask, is_causal = False)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [01:40<00:00, 14.38s/it]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "\n",
    "# Assumendo che test_dataset sia una lista di dizionari con 'prompt' e 'true_text'\n",
    "batch_size = 16  # Numero di esempi da processare in parallelo\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "def generate_batch_texts(prompts, model, tokenizer, max_length=4096):\n",
    "    \n",
    "    # Tokenizzazione dei prompt\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to('cuda')\n",
    "    \n",
    "    # Generazione del testo\n",
    "    outputs = model.generate(**inputs, max_length=max_length)\n",
    "    \n",
    "    # Decodifica del testo generato\n",
    "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "    # Rimozione del testo prima di \"Answer: \" in ogni prompt\n",
    "    modified_generated_output = []\n",
    "    for text in generated_texts:\n",
    "        modified_output = text.split(\"Answer: \", 1)[1]\n",
    "        modified_output = remove_punctuation(modified_output.lower()).strip()\n",
    "        modified_generated_output.append(modified_output)\n",
    "\n",
    "    answers_ids = tokenizer(modified_generated_output, return_tensors=\"pt\", padding=True, truncation=True).to('cuda')\n",
    "    answers_ids = answers_ids['input_ids']\n",
    "\n",
    "    # Rimozione degli elementi 0 e 1 da ciascun tensore nella lista\n",
    "    cleaned_input_ids = []\n",
    "    for tensor in answers_ids:\n",
    "        # Filtra il tensore per mantenere solo gli ID diversi da 0 e 1\n",
    "        filtered_tensor = tensor[tensor > 1]\n",
    "        cleaned_input_ids.append(filtered_tensor)\n",
    "\n",
    "    return cleaned_input_ids, modified_generated_output\n",
    "\n",
    "def generate_true_ids(prompts, model, tokenizer, max_length=4096):\n",
    "    # Tokenizzazione dei prompt\n",
    "    prompts = [remove_punctuation(prompt.lower()) for prompt in prompts]\n",
    "    outputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to('cuda')\n",
    "    outputs = outputs['input_ids']\n",
    "    \n",
    "    # Rimozione degli elementi 0 e 1 da ciascun tensore nella lista\n",
    "    cleaned_input_ids = []\n",
    "    for tensor in outputs:\n",
    "        # Filtra il tensore per mantenere solo gli ID diversi da 0 e 1\n",
    "        filtered_tensor = tensor[tensor > 1]\n",
    "        cleaned_input_ids.append(filtered_tensor)\n",
    "    \n",
    "    return cleaned_input_ids\n",
    "\n",
    "# Dataloader per creare batch di input\n",
    "data_loader = DataLoader(eval_dataset, batch_size=batch_size)\n",
    "\n",
    "# List per salvare i risultati\n",
    "prompts = []\n",
    "generated_texts = []\n",
    "# generated_ids = []\n",
    "true_texts = []\n",
    "# true_ids = []\n",
    "\n",
    "# Loop attraverso i batch\n",
    "for batch in tqdm(data_loader):\n",
    "    prompts.extend(batch['prompt'])  # Lista di prompt nel batch\n",
    "    true_texts.extend([remove_punctuation(el.lower()) for el in batch['completion']])  # Salva i true_texts per questo batch\n",
    "    \n",
    "    # Genera il testo per l'intero batch\n",
    "    batch_generated_ids, batch_generated_texts = generate_batch_texts(batch['prompt'], model, tokenizer)\n",
    "    batch_true_ids = generate_true_ids(batch['completion'], model, tokenizer)\n",
    "\n",
    "    # Salva i testi generati\n",
    "    generated_texts.extend(batch_generated_texts)\n",
    "    # generated_ids.extend(batch_generated_ids)\n",
    "    # true_ids.extend(batch_true_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "december 1  1996\n",
      "december 1\n"
     ]
    }
   ],
   "source": [
    "index = -1\n",
    "print(generated_texts[index])\n",
    "print(true_texts[index])\n",
    "# print(generated_ids[index])\n",
    "# print(true_ids[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:11<00:00,  3.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 32.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 11.11 seconds, 9.00 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import evaluate\n",
    "from bert_score import score\n",
    "\n",
    "def match_tokens(generated_texts_list, true_texts_list):\n",
    "    tp = 0  # Veri positivi\n",
    "    fp = 0  # Falsi positivi\n",
    "    fn = 0  # Falsi negativi\n",
    "\n",
    "    # Itera su ogni coppia di tensori nella lista\n",
    "    for true_list, generated_list in zip(true_texts_list, generated_texts_list):        \n",
    "        # Conta la frequenza di ciascun token nelle liste\n",
    "        true_counter = Counter(true_list)\n",
    "        generated_counter = Counter(generated_list)\n",
    "        \n",
    "        # Calcola i veri positivi e i falsi negativi\n",
    "        for token in true_counter:\n",
    "            if token in generated_counter:\n",
    "                tp += min(true_counter[token], generated_counter[token])\n",
    "                fn += max(0, true_counter[token] - generated_counter[token])\n",
    "            else:\n",
    "                fn += true_counter[token]\n",
    "        \n",
    "        # Calcola i falsi positivi\n",
    "        for token in generated_counter:\n",
    "            if token not in true_counter:\n",
    "                fp += generated_counter[token]\n",
    "            elif generated_counter[token] > true_counter[token]:\n",
    "                fp += generated_counter[token] - true_counter[token]\n",
    "    \n",
    "    return tp, fp, fn\n",
    "\n",
    "def compute_partial_match(generated_texts, true_texts):\n",
    "    pm = 0\n",
    "\n",
    "    # Itera su ogni coppia di tensori nella lista\n",
    "    for true_list, generated_list in zip(generated_texts, true_texts):        \n",
    "        # Conta la frequenza di ciascun token nelle liste\n",
    "        true_counter = Counter(true_list)\n",
    "        generated_counter = Counter(generated_list)\n",
    "\n",
    "        # Calcola i falsi positivi\n",
    "        for token in generated_counter:\n",
    "            if token in true_counter:\n",
    "                pm += 1\n",
    "                break\n",
    "    \n",
    "    return pm/len(true_texts)\n",
    "\n",
    "def compute_partial_match_jaccard(generated_texts, true_texts):\n",
    "    pmj = 0\n",
    "\n",
    "    # Itera su ogni coppia di tensori nella lista\n",
    "    for true_list, generated_list in zip(true_texts, generated_texts):     \n",
    "        true_set = set(true_list)   \n",
    "        generated_set = set(generated_list)\n",
    "        \n",
    "        pmj += len(true_set.intersection(generated_set)) / len(true_set.union(generated_set))\n",
    "        \n",
    "    return pmj/len(true_texts)\n",
    "\n",
    "def compute_exact_match(generated_texts, true_texts):\n",
    "    em = sum([1 if p == l else 0 for p, l in zip(generated_texts, true_texts)]) / len(true_texts)\n",
    "    return em\n",
    "\n",
    "def compute_rouge(generated_texts, true_texts):\n",
    "    rouge = evaluate.load('rouge')\n",
    "    results = rouge.compute(predictions=generated_texts,\n",
    "                            references=true_texts,\n",
    "                            use_aggregator=True)\n",
    "    return results\n",
    "\n",
    "def compute_bleu(generated_texts, true_texts):\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    true_list = [[text] for text in true_texts]\n",
    "    results = bleu.compute(predictions=generated_texts, references=true_list)\n",
    "    return results['bleu']\n",
    "\n",
    "def compute_bert_score(generated_texts, true_texts):\n",
    "    P, R, F1 = score(generated_texts, true_texts, lang='en', verbose=True)\n",
    "    return P.mean().item(), R.mean().item(), F1.mean().item()\n",
    "\n",
    "def compute_metrics(generated_texts, true_texts):\n",
    "    # Calculate Exact Match (EM)\n",
    "    em = compute_exact_match(generated_texts, true_texts) \n",
    "\n",
    "    # Calculate Partial Match (PM)\n",
    "    pm = compute_partial_match(generated_texts, true_texts)\n",
    "\n",
    "    # Calculate Partial Match Jaccard (PMJ)\n",
    "    pmj = compute_partial_match_jaccard(generated_texts, true_texts)\n",
    "\n",
    "    # Calculate F1\n",
    "    tp, fp, fn = match_tokens(generated_texts, true_texts)\n",
    "\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    # Calculate ROUGE\n",
    "    rouge_dict = compute_rouge(generated_texts, true_texts)\n",
    "\n",
    "    # Calculate BLEU\n",
    "    bleu = compute_bleu(generated_texts, true_texts)\n",
    "\n",
    "    # Compute Precision, Recall and F1 via BERT Score\n",
    "    bert_precision, bert_recall, bert_f1 = compute_bert_score(generated_texts, true_texts)\n",
    "\n",
    "    return {'exact_match' : em, 'partial_match' : pm, 'jaccard_partial_match' : pmj, \n",
    "            'precision' : precision, 'recall' : recall, 'f1' : f1,\n",
    "            'rouge1' : rouge_dict['rouge1'], 'rouge2' : rouge_dict['rouge2'], 'rougeL' :rouge_dict['rougeL'],\n",
    "            'rougeLsum': rouge_dict['rougeLsum'], 'bleu' : bleu,\n",
    "            'bert_precision' : bert_precision, 'bert_recall' : bert_recall, 'bert_f1' : bert_f1}\n",
    "\n",
    "# metrics = compute_metrics(generated_ids, true_ids)\n",
    "metrics = compute_metrics(generated_texts, true_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 0.51,\n",
       " 'partial_match': 0.98,\n",
       " 'jaccard_partial_match': 0.7867234848484849,\n",
       " 'precision': 0.7995512341062079,\n",
       " 'recall': 0.7842993396918562,\n",
       " 'f1': 0.7918518518518517,\n",
       " 'rouge1': 0.6902316017316016,\n",
       " 'rouge2': 0.45607142857142857,\n",
       " 'rougeL': 0.6883867243867242,\n",
       " 'rougeLsum': 0.6913152958152957,\n",
       " 'bleu': 0.43485823399845347,\n",
       " 'bert_precision': 0.9416860938072205,\n",
       " 'bert_recall': 0.9391191005706787,\n",
       " 'bert_f1': 0.9399850964546204}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\paolo\\_netrc\n"
     ]
    }
   ],
   "source": [
    "!wandb login 372f5c298afc4be9b40dd7b97523d394c3d30d05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"]=\"lost-in-the-middle\"\n",
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME \"]=\"lost-in-the-middle\"\n",
    "# save your trained model checkpoint to wandb\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"true\"\n",
    "# turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 3060 Ti. Max memory = 7.999 GB.\n",
      "7.273 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 3\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    eval_dataset[index][\"prompt\"]\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "# print(eval_dataset[2][\"prompt\"])\n",
    "outputs = model.generate(**inputs, use_cache = True)\n",
    "print(tokenizer.batch_decode(outputs)[0])\n",
    "print(\"-------------\")\n",
    "print(\"Risposta esatta: \", eval_dataset[index][\"completion\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
