{"cells":[{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["d:\\anaconda3\\envs\\lost_in_the_middle\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import polars as pl\n","from pathlib import Path\n","from datasets import load_dataset, concatenate_datasets\n","from tqdm.auto import tqdm\n","import torch\n","from joblib import Parallel, delayed\n","import os"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Caricamento Dataset di partenza\n","# golden_dataset = load_dataset('florin-hf/nq_open_gold')\n","# # train_golden_dataset = golden_dataset['test']\n","# test_dataset = golden_dataset['test']\n","# evaluation_dataset = golden_dataset['validation']\n","\n","# # Unisci i dataset\n","# train_golden_dataset = concatenate_datasets([test_dataset, evaluation_dataset])\n","\n","# Caricamento dataset con embeddings già calcolati\n","train_golden_dataset = pl.read_parquet('data/embeddings_nq_gold_1.parquet')\n","nq_embeddings = train_golden_dataset['Embeddings'].to_list()\n","nq_embeddings = torch.tensor(nq_embeddings)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["#Caricamento Dataset Corpus \n","# df = pl.read_parquet(\"data/corpus.parquet\")\n","\n","# # # Tolgo gold\n","# idx_list = train_golden_dataset[\"idx_gold_in_corpus\"]\n","# no_gold_df = df.with_row_index(\"row_num\").filter(pl.col(\"row_num\").is_in(idx_list).not_())\n","\n","# sampled_corpus_df = no_gold_df.sample(n=1000000)\n","\n","############################################\n","\n","#Caricamento Dataset Corpus con già embeddings\n","sampled_corpus_df = pl.read_parquet('data/embeddings_corpus_1000000_1.parquet')\n","embeddings = sampled_corpus_df['Embeddings'].to_list()\n","embeddings = torch.tensor(embeddings)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["df = pl.read_parquet(\"data/corpus.parquet\")\n","idx_list = train_golden_dataset[\"idx_gold_in_corpus\"]\n","gold_df = df.with_row_index(\"row_num\").filter(pl.col(\"row_num\").is_in(idx_list))"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["from sentence_transformers import SentenceTransformer\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","print(device)\n","sbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 1000000/1000000 [2:41:02<00:00, 103.49it/s] \n"]}],"source":["text_list = sampled_corpus_df[\"Text\"].to_list()\n","embeddings = []\n","\n","for text in tqdm(text_list):\n","    embeddings.append(sbert_model.encode(text))"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","embeddings_series = pl.Series(\"Embeddings\", embeddings)\n","sampled_corpus_df = sampled_corpus_df.with_columns(embeddings_series)\n","sampled_corpus_df.write_parquet('data/embeddings_corpus_1000000_1.parquet')\n","\n","# all_embeddings = np.array(embeddings)\n","# np.save('data/embeddings.npy', all_embeddings)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 10895/10895 [01:39<00:00, 109.12it/s]\n"]}],"source":["nq_text_list = train_golden_dataset[\"text\"]\n","nq_embeddings = []\n","\n","# n_jobs = os.cpu_count() - 1\n","# nq_embeddings = Parallel(n_jobs=n_jobs, verbose=10)(delayed(sbert_model.encode)(text) for text in nq_text_list)\n","\n","for text in tqdm(nq_text_list):\n","    nq_embeddings.append(sbert_model.encode(text))"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Creating parquet from Arrow format: 100%|██████████| 11/11 [00:00<00:00, 34.05ba/s]\n"]},{"data":{"text/plain":["23865175"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","\n","train_golden_dataset = train_golden_dataset.add_column(\"Embeddings\", nq_embeddings)\n","train_golden_dataset.to_parquet('data/embeddings_nq_gold_1.parquet')\n","\n","# all_embeddings = np.array(nq_embeddings)\n","# np.save('data/nq_embeddings.npy', all_embeddings)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["import faiss \n","\n","d = embeddings.shape[1]\n","n_bits = 2*d\n","index = faiss.IndexLSH(d, n_bits)\n","\n","res = faiss.StandardGpuResources()\n","gpu_index_flat = faiss.index_cpu_to_gpu(res, 0, index)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1000000\n"]}],"source":["faiss.normalize_L2(embeddings.numpy())\n","gpu_index_flat.add(embeddings) \n","print(gpu_index_flat.ntotal)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([10895, 384])\n"]}],"source":["# test_embedding = nq_embeddings[0].reshape(1, d)\n","faiss.normalize_L2(nq_embeddings.numpy())\n","print(nq_embeddings.shape)\n","k = 100\n","D, I = gpu_index_flat.search(nq_embeddings, k)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 10895/10895 [00:07<00:00, 1499.24it/s]\n"]}],"source":["from sklearn.metrics.pairwise import cosine_similarity\n","\n","similarity_matrix = []\n","for i, documents_idx in enumerate(tqdm(I)):\n","    similarity_matrix.append(cosine_similarity(embeddings[documents_idx], nq_embeddings[i].reshape(1, -1)))"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["import numpy as np\n","similarity_matrix = np.array(similarity_matrix).squeeze()"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["def top_k_indices_per_row(matrix, k):\n","    # Get the indices that would sort each row\n","    sorted_indices = np.argsort(matrix, axis=1)[:, ::-1]\n","\n","    # Take the first 7 indices from each row (corresponding to the highest values)\n","    top_k_indices = sorted_indices[:, :k]\n","\n","    return top_k_indices"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["related_dataset = []\n","gold_document_position = 7 # @param {type:\"slider\", min:0, max:7, step:1}\n","num_documents = 8\n","num_related_documents = num_documents - 1\n","random_gold_position = True"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(10895, 7)\n"]}],"source":["top_7_indexes_of_I = top_k_indices_per_row(similarity_matrix, num_related_documents)\n","\n","top_7_idexes_of_documents = []\n","for document in range(len(top_7_indexes_of_I)):\n","    indici = top_7_indexes_of_I[document]\n","    top_7_idexes_of_documents.append(I[document][indici])\n","\n","top_7_idexes_of_documents = np.array(top_7_idexes_of_documents)\n","print(top_7_idexes_of_documents.shape)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/plain":["10350"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["len(gold_df)"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 10895/10895 [00:13<00:00, 802.64it/s] "]},{"name":"stdout","output_type":"stream","text":["16664\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["import random\n","\n","for idx in tqdm(range(len(train_golden_dataset))):\n","    question = {}\n","    if random_gold_position:\n","        gold_document_position = random.randint(0, 7)\n","    question['Question'] = train_golden_dataset[idx][\"question\"][0]\n","    idx_gold_in_corpus = train_golden_dataset[idx][\"idx_gold_in_corpus\"][0]\n","    question['Answers'] = train_golden_dataset[idx][\"answers\"][0]\n","    question['Golden_idx'] = gold_document_position\n","    gold_element = gold_df.filter(pl.col(\"row_num\").eq(idx_gold_in_corpus))\n","    if len(gold_element) > 0:\n","        question['Documents'] = [None] * (num_documents)\n","        # Insert Gold Document\n","        document = {}\n","        document['Title'] = gold_element['Title'][0]\n","        document['Text'] = gold_element['Text'][0]\n","        question['Documents'][gold_document_position] = document\n","\n","        \n","        list_of_corpus_indexes = top_7_idexes_of_documents[idx]\n","        # Insert other random Documents\n","        df_idx = 0\n","        for i in range(len(question['Documents'])):\n","            document = {}\n","            if i == gold_document_position:\n","                continue\n","            corpus_element = sampled_corpus_df.row(list_of_corpus_indexes[df_idx])\n","            \n","            document[\"Title\"] = corpus_element[1]\n","            document[\"Text\"] = corpus_element[2]\n","            question['Documents'][i] = document\n","            df_idx += 1\n","\n","        related_dataset.append(question)\n","    # print(\"Terminato idx:\", idx)\n","\n","print(len(related_dataset))"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"data":{"text/plain":["{'Question': 'when is the next deadpool movie being released',\n"," 'Answers': shape: (1,)\n"," Series: '' [str]\n"," [\n"," \t\"May 18 , 2018\"\n"," ],\n"," 'Golden_idx': 6,\n"," 'Documents': [{'Title': 'Deadpool 2',\n","   'Text': 'Deadpool 2 Deadpool 2 is a 2018 American superhero film based on the Marvel Comics character Deadpool, distributed by 20th Century Fox. It is the eleventh installment in the \"X-Men\" film series, and a direct sequel to the 2016 film \"Deadpool\". The film is directed by David Leitch from a script by Rhett Reese, Paul Wernick, and Ryan Reynolds, with Reynolds starring in the title role alongside Josh Brolin, Morena Baccarin, Julian Dennison, Zazie Beetz, T.J. Miller, Brianna Hildebrand, and Jack Kesy. In the film, Deadpool forms the team X-Force to protect a young mutant from the time-traveling soldier Cable.'},\n","  {'Title': 'Deadpool (film)',\n","   'Text': 'Deadpool film starring Reynolds began in February 2004, before he went on to play the character in \"\" in 2009. Reese and Wernick were hired for a spinoff in 2010. They worked with Reynolds to adapt the character more faithfully (including his fourth wall breaking) after the portrayal in \"Wolverine\" was criticized for not doing so. Miller was hired in 2011 marking his directorial debut. An enthusiastic response to leaked test footage he created with Reynolds led to a green-light from Fox in 2014. Additional casting began in early 2015, and filming took place in Vancouver, Canada, from March to'},\n","  {'Title': 'Deadpool 2',\n","   'Text': 'returned to direct them during a single day in August. Wernick said the version would not just be for children who were unable to watch the R-rated release, as \"it\\'s subversive enough and fun and creative and something that only \"Deadpool\" could do. So I think it\\'s going to be a real joy for not only a whole new audience, but also an audience that has seen and loved the \"Deadpool\" movies.\" They added that the film\\'s story would not change \"appreciably\" between versions. The majority of the version is the same as the theatrical release, but edited to \"meet'},\n","  {'Title': 'Deadpool 2 (soundtrack)',\n","   'Text': 'treats: an actually funny action-film soundtrack\". \"Deadpool 2 (Original Motion Picture Soundtrack)\" was nominated for the Grammy Award for Best Compilation Soundtrack for Visual Media at the 61st Annual Grammy Awards. If won, the award will be presented to the compilation producers David Leitch and Ryan Reynolds, and to the music supervisor, John Houlihan. Deadpool 2 (soundtrack) The soundtrack for the 2018 American superhero film \"Deadpool 2\", based on the Marvel Comics character Deadpool and distributed by 20th Century Fox, consists of an original score composed by Tyler Bates and a series of songs featured in the film. This includes'},\n","  {'Title': 'Digimon Adventure tri.',\n","   'Text': '7, 2017. The second film, , was released in Japan on March 12, 2016 as well as Germany and Austria on July 2, 2017. It was released on region-free DVD and Blu-ray in Japan on April 2, 2016, the U.S. at Anime Expo from July 1, 2017 through July 4, 2017 as well as San Diego Comic-Con from July 19, 2017 through July 23, 2017, ahead of a general release on August 15, 2017, Germany on October 9, 2017, the UK at MCM London Comic Con from October 27, 2017 through October 29, 2017, ahead of a general release on'},\n","  {'Title': 'The Crown (TV series)',\n","   'Text': 'a series of invented scenes bearing no relation to the truth.\" The series\\'s first two episodes were released theatrically in the United Kingdom on November 1, 2016. The first season was released worldwide in its entirety on November 4, 2016. The second season was released on December 8, 2017. The third season is expected to be released in 2019. Season 1 was released on DVD and Blu-ray in the United Kingdom on October 16, 2017 and released worldwide on November 7, 2017. Season 2 was released on DVD and Blu-ray in the United Kingdom on October 22, 2018 and was'},\n","  {'Title': 'Deadpool 2',\n","   'Text': 'Deadpool 2 is scheduled to be released in the United States on May 18 , 2018 . A sequel , Deadpool 3 , is in development .'},\n","  {'Title': 'Justice League (film)',\n","   'Text': 'Justice League held its world premiere in Beijing on October 26 , 2017 , and was theatrically released in North America in standard , RealD 3D and IMAX on November 17 , 2017 . Justice League was released on Digital HD on February 13 , 2018 , and is scheduled to be released on Blu - ray , Blu - ray 3D , 4K Ultra-HD Blu - ray and DVD on March 13 , 2018 .'}]}"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["related_dataset[1]"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["random_dataset_df = pl.DataFrame(related_dataset)\n","path = f\"data/related_dataset_gold_at_random_position_1.parquet\"\n","random_dataset_df.write_parquet(path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["new_df = pl.DataFrame(related_dataset)\n","\n","old_df = pl.read_parquet('data/related_dataset_gold_at_random_position.parquet')\n","path = f\"data/related_dataset_gold_at_random_position_def.parquet\"\n","\n","combinated = com\n","random_dataset_df.write_parquet(path)"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["def prepare_data(data, split):\n","    processed_data = []\n","    for entry in data:\n","        question = entry['Question']\n","        documents = entry['Documents']\n","\n","        # Creare la stringa dei documenti\n","        docs_str = \"\"\n","        for i, doc in enumerate(documents):\n","            docs_str += f\"Document [{i}](Title: {doc['Title']}) {doc['Text']}\\n\"\n","\n","        # Creare la stringa completa del prompt\n","        prompt = (f\"You are given a question and you MUST respond by EXTRACTING the answer \"\n","                  f\"(max 5 tokens) from one of the provided documents. If none of the documents contain \"\n","                  f\"the answer, respond with NO-RES.\\nDocuments:\\n{docs_str}Question: {question}\\nAnswer:\")\n","\n","        processed_data.append({\"prompt\": prompt, \"completion\": entry['Answers'][0]})\n","        # processed_data.append({\"text\": prompt})\n","\n","    return processed_data"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["57767\n","14442\n"]}],"source":["# Shuffle data and split into train and test\n","import pandas as pd\n","import random\n","\n","df = pd.read_parquet('data/related_dataset_gold_at_7.parquet')\n","data = df.to_dict(orient='records')\n","random.shuffle(data)\n","split_idx = int(len(data) * 0.8)\n","train_data = prepare_data(data[:split_idx], 'train')\n","test_data = prepare_data(data[split_idx:], 'test')\n","print(len(train_data))\n","print(len(test_data))"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["\n","from datasets import DatasetDict, Dataset\n","dataset_dict = DatasetDict({\n","    \"train\": Dataset.from_list(train_data),\n","    \"test\": Dataset.from_list(test_data)\n","})\n","\n","# Salva il dataset\n","dataset_dict.save_to_disk(\"output_dataset\")"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Creating parquet from Arrow format: 100%|██████████| 58/58 [00:01<00:00, 42.81ba/s]\n","Uploading the dataset shards: 100%|██████████| 1/1 [00:09<00:00,  9.45s/it]\n","Creating parquet from Arrow format: 100%|██████████| 15/15 [00:00<00:00, 47.21ba/s]\n","Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.12s/it]\n"]},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/datasets/Paoloc99/related_dataset_gold_at_7/commit/a009e19a4ce9d64103fb9ac4a5166337bbdfa4cc', commit_message='Upload dataset', commit_description='', oid='a009e19a4ce9d64103fb9ac4a5166337bbdfa4cc', pr_url=None, pr_revision=None, pr_num=None)"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["# Autenticazione con Huggingface\n","dataset_dict.push_to_hub(\"Paoloc99/related_dataset_gold_at_7\", token=\"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\")"]}],"metadata":{"kernelspec":{"display_name":"power_of_noise","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"}},"nbformat":4,"nbformat_minor":2}
