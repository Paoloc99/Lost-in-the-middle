{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["d:\\anaconda3\\envs\\power_of_noise\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import polars as pl\n","from pathlib import Path\n","from datasets import load_dataset\n","from tqdm.auto import tqdm\n","import torch"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# golden_dataset = load_dataset('florin-hf/nq_open_gold')\n","# train_golden_dataset = golden_dataset['train']\n","\n","train_golden_dataset = pl.read_parquet('data/embeddings_nq_gold.parquet')\n","nq_embeddings = train_golden_dataset['Embeddings'].to_list()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# df = pl.read_parquet(\"data/corpus.parquet\")\n","\n","# # Tolgo gold\n","# idx_list = train_golden_dataset[\"idx_gold_in_corpus\"]\n","# no_gold_df = df.with_row_index(\"row_num\").filter(pl.col(\"row_num\").is_in(idx_list).not_())\n","\n","# sampled_corpus_df = no_gold_df.sample(n=1000000)\n","\n","sampled_corpus_df = pl.read_parquet('data/embeddings_corpus_1000000.parquet')\n","embeddings = sampled_corpus_df['Embeddings'].to_list()"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["from sentence_transformers import SentenceTransformer\n","from joblib import Parallel, delayed\n","import os\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","print(device)\n","sbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["random_dataset = []\n","gold_document_position = 7 # @param {type:\"slider\", min:0, max:7, step:1}\n","num_documents = 8\n","num_random_documents = num_documents - 1"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 1000000/1000000 [2:11:38<00:00, 126.61it/s] \n"]}],"source":["text_list = sampled_corpus_df[\"Text\"].to_list()\n","embeddings = []\n","\n","# n_jobs = os.cpu_count() - 1\n","# embeddings = Parallel(n_jobs=n_jobs, verbose=10)(delayed(sbert_model.encode)(text) for text in text_list)\n","\n","for text in tqdm(text_list):\n","    embeddings.append(sbert_model.encode(text))\n","\n","# embeddings = sbert_model.encode(text_list, batch_size=128)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","embeddings_series = pl.Series(\"Embeddings\", embeddings)\n","sampled_corpus_df = sampled_corpus_df.with_columns(embeddings_series)\n","sampled_corpus_df.write_parquet('data/embeddings_corpus_1000000.parquet')\n","\n","# all_embeddings = np.array(embeddings)\n","# np.save('data/embeddings.npy', all_embeddings)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 72209/72209 [09:27<00:00, 127.33it/s]\n"]}],"source":["nq_text_list = train_golden_dataset[\"text\"]\n","nq_embeddings = []\n","\n","# n_jobs = os.cpu_count() - 1\n","# nq_embeddings = Parallel(n_jobs=n_jobs, verbose=10)(delayed(sbert_model.encode)(text) for text in nq_text_list)\n","\n","for text in tqdm(nq_text_list):\n","    nq_embeddings.append(sbert_model.encode(text))"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Creating parquet from Arrow format: 100%|██████████| 73/73 [00:01<00:00, 41.57ba/s]\n"]},{"data":{"text/plain":["158415051"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","\n","train_golden_dataset = train_golden_dataset.add_column(\"Embeddings\", nq_embeddings)\n","train_golden_dataset.to_parquet('data/embeddings_nq_gold.parquet')\n","\n","# all_embeddings = np.array(nq_embeddings)\n","# np.save('data/nq_embeddings.npy', all_embeddings)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["from sklearn.metrics.pairwise import cosine_similarity\n","from sortedcontainers import SortedList\n","\n","def top_k_indices_per_row(matrix, k):\n","    # Get the indices that would sort each row\n","    sorted_indices = np.argsort(matrix, axis=1)[:, ::-1]\n","\n","    # Take the first 7 indices from each row (corresponding to the highest values)\n","    top_k_indices = sorted_indices[:, :k]\n","\n","    return top_k_indices\n","\n","def top_k_cosine_similarity_indexes(gold, documents = embeddings, top_K=7):\n","    similarity = cosine_similarity([gold], documents)\n","    idx_list = top_k_indices_per_row(similarity, top_K)\n","    return idx_list\n","        "]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["import multiprocessing as mp\n","from datetime import datetime\n","from math import pow, sqrt\n","\n","class SimilarityMetric():\n","\tdef __init__(self, processes):\n","\t\tself.processes = processes\n","\t\tpass\n","\n","\t#COSINE SIMILARITY\n","\t\n","\t#serial cosine similarity\n","\tdef square_rooted(self, x):\n","\t\treturn round(sqrt(sum([a*a for a in x])),3)\n","\t\n","\tdef serial_cosine_similarity(self,x,y):\n","\t\tnumerator = sum(a*b for a,b in zip(x,y))\n","\t\tdenominator = self.square_rooted(x)*self.square_rooted(y)\n","\t\treturn round(numerator/float(denominator),3)\n","\t\n","\tdef multplierr(self,a,b):\n","\t\t\treturn a*b\n","\t\n","\t#parallel cosine similarity\n","\tdef parallel_cosine_similarity(self,x,y):\n","\n","\t\tpool = mp.Pool(processes= self.processes)\n","\t\ts = datetime.now()\n","\t\tnums = pool.starmap(self.multplierr, zip(x,y))\n","\t\tnumerator = sum(nums)\n","\t\t\n","\t\t#x_sqr = pool.starmap( self.multplierr, zip(x,x))\n","\t\t#y_sqr = pool.starmap( self.multplierr, zip(y,y))\n","\t\t\n","\t\t#denominator = round(sqrt(sum(x_sqr))) * round(sqrt(sum(y_sqr)))\n","\t\tdenominator = self.square_rooted(x)*self.square_rooted(y)\n","\t\t\n","\t\te = datetime.now()\n","\t\tprint(\"Parallel Cosine Exec Time: \", e-s)\n","\t\treturn round(numerator/float(denominator),3)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sm = SimilarityMetric(11)\n","\n","half = len(nq_embeddings)//2\n","gold_embeddings_first_half = nq_embeddings[:half]\n","# gold_embeddings_second_half = nq_embeddings[half:]\n","first_half_similarity = sm.parallel_cosine_similarity(gold_embeddings_first_half, embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["n_jobs = os.cpu_count() - 2\n","half = len(nq_embeddings)//2\n","gold_embeddings_first_half = nq_embeddings[:half]\n","# gold_embeddings_second_half = nq_embeddings[half:]\n","print(len(gold_embeddings_first_half))\n","list_top_k_indexes = Parallel(n_jobs=n_jobs, verbose=10)(delayed(top_k_cosine_similarity_indexes)(emb, top_K=num_random_documents) for emb in gold_embeddings_first_half)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics.pairwise import cosine_similarity\n","\n","# Unable to allocate 315 GiB\n","# similarity_matrix = cosine_similarity(nq_embeddings, embeddings)\n","\n","# print(cosine_similarity(nq_embeddings[0].reshape(1, -1), nq_embeddings[0].reshape(1, -1)))\n","\n","# 32 ore\n","similarity_matrix = []\n","for embedding in tqdm(nq_embeddings):\n","    embedding_similarity = cosine_similarity([embedding], embeddings)\n","    similarity_matrix.append(embedding_similarity)\n","\n","# print(len(similarity_matrix))\n","# Trova i 7 indici con le similarità più alte per ogni embedding di partenza\n","# top_k = 7\n","# top_k_indices = np.argsort(-similarity_matrix, axis=1)[:, :top_k]  # Ordina in ordine decrescente\n","\n","# # Stampa i risultati\n","# for i, query in enumerate(nq_embeddings_test):\n","#     print(f\"Query embedding {i}:\")\n","#     print(f\"Indices of 7 most similar embeddings: {top_k_indices[i]}\")\n","#     print(f\"Similarity scores: {similarity_matrix[i, top_k_indices[i]]}\")\n","#     print()"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["72209\n"]}],"source":["for idx in range(len(train_golden_dataset)):\n","    question = {}\n","    question['Question'] = train_golden_dataset[idx][\"question\"]\n","    idx_gold_in_corpus = train_golden_dataset[idx][\"idx_gold_in_corpus\"]\n","    question['Answers'] = train_golden_dataset[idx][\"answers\"]\n","    question['Golden_idx'] = gold_document_position\n","    corpus_element = df.row(idx_gold_in_corpus)\n","    sampled_elements = df.sample(n=num_random_documents)\n","    question['Documents'] = [None] * (num_documents)\n","    \n","    # Insert Gold Document\n","    document = {}\n","    document['Title'] = corpus_element[0]\n","    document['Text'] = corpus_element[1]\n","    question['Documents'][gold_document_position] = document\n","    # Insert other random Documents\n","    df_idx = 0\n","    for i in range(len(question['Documents'])):\n","        document = {}\n","        if i == gold_document_position:\n","            continue\n","        document[\"Title\"] = sampled_elements[df_idx][\"Title\"][0]\n","        document[\"Text\"] = sampled_elements[df_idx][\"Text\"][0]\n","        df_idx += 1\n","        question['Documents'][df_idx] = document\n","\n","    random_dataset.append(question)\n","    # print(\"Terminato idx:\", idx)\n","\n","print(len(random_dataset))"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[],"source":["random_dataset_df = pl.DataFrame(random_dataset)\n","path = f\"data/random_dataset_gold_at_{gold_document_position}.parquet\"\n","random_dataset_df.write_parquet(path)"]}],"metadata":{"kernelspec":{"display_name":"power_of_noise","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"}},"nbformat":4,"nbformat_minor":2}
