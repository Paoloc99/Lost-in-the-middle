{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["d:\\anaconda3\\envs\\lost_in_the_middle\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import polars as pl\n","from pathlib import Path\n","from datasets import load_dataset\n","from tqdm.auto import tqdm\n","import torch\n","from joblib import Parallel, delayed\n","import os"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# golden_dataset = load_dataset('florin-hf/nq_open_gold')\n","# train_golden_dataset = golden_dataset['train']\n","\n","train_golden_dataset = pl.read_parquet('data/embeddings_nq_gold.parquet')\n","nq_embeddings = train_golden_dataset['Embeddings'].to_list()\n","nq_embeddings = torch.tensor(nq_embeddings)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# df = pl.read_parquet(\"data/corpus.parquet\")\n","\n","# # Tolgo gold\n","# idx_list = train_golden_dataset[\"idx_gold_in_corpus\"]\n","# no_gold_df = df.with_row_index(\"row_num\").filter(pl.col(\"row_num\").is_in(idx_list).not_())\n","\n","# sampled_corpus_df = no_gold_df.sample(n=1000000)\n","\n","sampled_corpus_df = pl.read_parquet('data/embeddings_corpus_1000000.parquet')\n","embeddings = sampled_corpus_df['Embeddings'].to_list()\n","embeddings = torch.tensor(embeddings)"]},{"cell_type":"code","execution_count":199,"metadata":{},"outputs":[],"source":["df = pl.read_parquet(\"data/corpus.parquet\")\n","idx_list = train_golden_dataset[\"idx_gold_in_corpus\"]\n","gold_df = df.with_row_index(\"row_num\").filter(pl.col(\"row_num\").is_in(idx_list))"]},{"cell_type":"code","execution_count":203,"metadata":{},"outputs":[{"data":{"text/html":["<div><style>\n",".dataframe > thead > tr,\n",".dataframe > tbody > tr {\n","  text-align: right;\n","  white-space: pre-wrap;\n","}\n","</style>\n","<small>shape: (1, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>row_num</th><th>Title</th><th>Text</th></tr><tr><td>u32</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>20970735</td><td>&quot;List of death row inmates in t…</td><td>&quot;As of June 14 , 2018 , there w…</td></tr></tbody></table></div>"],"text/plain":["shape: (1, 3)\n","┌──────────┬─────────────────────────────────┬─────────────────────────────────┐\n","│ row_num  ┆ Title                           ┆ Text                            │\n","│ ---      ┆ ---                             ┆ ---                             │\n","│ u32      ┆ str                             ┆ str                             │\n","╞══════════╪═════════════════════════════════╪═════════════════════════════════╡\n","│ 20970735 ┆ List of death row inmates in t… ┆ As of June 14 , 2018 , there w… │\n","└──────────┴─────────────────────────────────┴─────────────────────────────────┘"]},"execution_count":203,"metadata":{},"output_type":"execute_result"}],"source":[]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["from sentence_transformers import SentenceTransformer\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","print(device)\n","sbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 1000000/1000000 [2:11:38<00:00, 126.61it/s] \n"]}],"source":["text_list = sampled_corpus_df[\"Text\"].to_list()\n","embeddings = []\n","\n","for text in tqdm(text_list):\n","    embeddings.append(sbert_model.encode(text))"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","embeddings_series = pl.Series(\"Embeddings\", embeddings)\n","sampled_corpus_df = sampled_corpus_df.with_columns(embeddings_series)\n","sampled_corpus_df.write_parquet('data/embeddings_corpus_1000000.parquet')\n","\n","# all_embeddings = np.array(embeddings)\n","# np.save('data/embeddings.npy', all_embeddings)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 72209/72209 [09:27<00:00, 127.33it/s]\n"]}],"source":["nq_text_list = train_golden_dataset[\"text\"]\n","nq_embeddings = []\n","\n","# n_jobs = os.cpu_count() - 1\n","# nq_embeddings = Parallel(n_jobs=n_jobs, verbose=10)(delayed(sbert_model.encode)(text) for text in nq_text_list)\n","\n","for text in tqdm(nq_text_list):\n","    nq_embeddings.append(sbert_model.encode(text))"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Creating parquet from Arrow format: 100%|██████████| 73/73 [00:01<00:00, 41.57ba/s]\n"]},{"data":{"text/plain":["158415051"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","\n","train_golden_dataset = train_golden_dataset.add_column(\"Embeddings\", nq_embeddings)\n","train_golden_dataset.to_parquet('data/embeddings_nq_gold.parquet')\n","\n","# all_embeddings = np.array(nq_embeddings)\n","# np.save('data/nq_embeddings.npy', all_embeddings)"]},{"cell_type":"code","execution_count":106,"metadata":{},"outputs":[],"source":["import faiss \n","\n","d = embeddings.shape[1]\n","n_bits = 2*d\n","index = faiss.IndexLSH(d, n_bits)\n","\n","res = faiss.StandardGpuResources()\n","gpu_index_flat = faiss.index_cpu_to_gpu(res, 0, index)"]},{"cell_type":"code","execution_count":107,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1000000\n"]}],"source":["faiss.normalize_L2(embeddings.numpy())\n","gpu_index_flat.add(embeddings) \n","print(gpu_index_flat.ntotal)"]},{"cell_type":"code","execution_count":136,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([72209, 384])\n"]}],"source":["# test_embedding = nq_embeddings[0].reshape(1, d)\n","faiss.normalize_L2(nq_embeddings.numpy())\n","print(nq_embeddings.shape)\n","k = 100\n","D, I = gpu_index_flat.search(nq_embeddings, k)"]},{"cell_type":"code","execution_count":146,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 72209/72209 [00:49<00:00, 1445.56it/s]\n"]}],"source":["from sklearn.metrics.pairwise import cosine_similarity\n","\n","similarity_matrix = []\n","for i, documents_idx in enumerate(tqdm(I)):\n","    similarity_matrix.append(cosine_similarity(embeddings[documents_idx], nq_embeddings[i].reshape(1, -1)))"]},{"cell_type":"code","execution_count":157,"metadata":{},"outputs":[],"source":["similarity_matrix = np.array(similarity_matrix).squeeze()"]},{"cell_type":"code","execution_count":158,"metadata":{},"outputs":[],"source":["def top_k_indices_per_row(matrix, k):\n","    # Get the indices that would sort each row\n","    sorted_indices = np.argsort(matrix, axis=1)[:, ::-1]\n","\n","    # Take the first 7 indices from each row (corresponding to the highest values)\n","    top_k_indices = sorted_indices[:, :k]\n","\n","    return top_k_indices\n","        "]},{"cell_type":"code","execution_count":259,"metadata":{},"outputs":[],"source":["related_dataset = []\n","gold_document_position = 7 # @param {type:\"slider\", min:0, max:7, step:1}\n","num_documents = 8\n","num_related_documents = num_documents - 1"]},{"cell_type":"code","execution_count":220,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(72209, 7)\n"]}],"source":["top_7_indexes_of_I = top_k_indices_per_row(similarity_matrix, num_related_documents)\n","\n","top_7_idexes_of_documents = []\n","for document in range(len(top_7_indexes_of_I)):\n","    indici = top_7_indexes_of_I[document]\n","    top_7_idexes_of_documents.append(I[document][indici])\n","\n","top_7_idexes_of_documents = np.array(top_7_idexes_of_documents)\n","print(top_7_idexes_of_documents.shape)"]},{"cell_type":"code","execution_count":260,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 72209/72209 [00:24<00:00, 2964.32it/s]"]},{"name":"stdout","output_type":"stream","text":["72209\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["for idx in tqdm(range(len(train_golden_dataset))):\n","    question = {}\n","    question['Question'] = train_golden_dataset[idx][\"question\"]\n","    idx_gold_in_corpus = train_golden_dataset[idx][\"idx_gold_in_corpus\"]\n","    question['Answers'] = train_golden_dataset[idx][\"answers\"]\n","    question['Golden_idx'] = gold_document_position\n","    gold_element = gold_df.filter(pl.col(\"row_num\").eq(idx_gold_in_corpus))\n","    question['Documents'] = [None] * (num_documents)\n","    \n","    # Insert Gold Document\n","    document = {}\n","    document['Title'] = gold_element['Title'][0]\n","    document['Text'] = gold_element['Text'][0]\n","    question['Documents'][gold_document_position] = document\n","\n","    \n","    list_of_corpus_indexes = top_7_idexes_of_documents[idx]\n","    # Insert other random Documents\n","    df_idx = 0\n","    for i in range(len(question['Documents'])):\n","        document = {}\n","        if i == gold_document_position:\n","            continue\n","        corpus_element = sampled_corpus_df.row(list_of_corpus_indexes[df_idx])\n","        \n","        document[\"Title\"] = corpus_element[1]\n","        document[\"Text\"] = corpus_element[2]\n","        question['Documents'][i] = document\n","        df_idx += 1\n","\n","    related_dataset.append(question)\n","    # print(\"Terminato idx:\", idx)\n","\n","print(len(related_dataset))"]},{"cell_type":"code","execution_count":262,"metadata":{},"outputs":[],"source":["random_dataset_df = pl.DataFrame(related_dataset)\n","path = f\"data/related_dataset_gold_at_{gold_document_position}.parquet\"\n","random_dataset_df.write_parquet(path)"]}],"metadata":{"kernelspec":{"display_name":"power_of_noise","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"}},"nbformat":4,"nbformat_minor":2}
